#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de Validation SuperSmartMatch V3.0 
Test des am√©liorations de pr√©cision m√©tier

üéØ OBJECTIFS DE VALIDATION :
- Tester la r√©solution des probl√®mes identifi√©s
- Comparer V2.1 vs V3.0 sur cas probl√©matiques
- Valider la granularit√© m√©tier fine
- Mesurer les performances

Auteur: SuperSmartMatch V3.0 Validation
"""

import requests
import json
import time
from typing import Dict, List, Any, Tuple
from datetime import datetime

class SuperSmartMatchV3Validator:
    """Validateur des am√©liorations V3.0"""
    
    def __init__(self, api_base: str = "http://localhost:5061/api/v1"):
        self.api_base = api_base
        self.results = {
            'test_cases': [],
            'v2_vs_v3_comparison': [],
            'performance_metrics': {},
            'validation_summary': {}
        }
        
        # Cas de test probl√©matiques identifi√©s
        self.problematic_test_cases = [
            {
                'name': 'Gestionnaire_Paie_vs_Assistant_Facturation',
                'description': 'üéØ CAS PRINCIPAL : Gestionnaire de paie ne doit PAS matcher avec Assistant facturation',
                'candidate': {
                    'titre_poste': 'Gestionnaire de Paie',
                    'competences': ['Sage Paie', 'DADS', 'Urssaf', 'Charges sociales', 'Bulletin de paie'],
                    'missions': [
                        'Gestion de la paie mensuelle pour 150 salari√©s',
                        '√âtablissement des bulletins de paie',
                        'D√©clarations sociales URSSAF',
                        'Suivi des charges sociales'
                    ],
                    'annees_experience': 3,
                    'secteur': 'ressources humaines'
                },
                'jobs': [
                    {
                        'id': 'assistant_facturation_001',
                        'titre': 'Assistant Facturation',
                        'competences': ['Facturation', 'Relances clients', 'Recouvrement', 'Excel'],
                        'missions': [
                            '√âmission des factures clients',
                            'Suivi des encaissements',
                            'Relances clients impay√©s',
                            'Reporting commercial'
                        ],
                        'secteur': 'comptabilit√©',
                        'description': 'Assistant facturation pour gestion client√®le'
                    }
                ],
                'expected_v3_score_max': 30,  # Doit √™tre tr√®s faible
                'expected_v2_score_issue': 'high'  # V2.1 avait un score √©lev√© probl√©matique
            },
            {
                'name': 'Assistant_Juridique_vs_Manager',
                'description': 'üéØ Assistant juridique ne doit PAS √™tre consid√©r√© comme Manager',
                'candidate': {
                    'titre_poste': 'Assistant Juridique',
                    'competences': ['Droit des affaires', 'R√©daction juridique', 'Veille juridique'],
                    'missions': [
                        'Assistance √† la r√©daction de contrats',
                        'Classement et archivage de dossiers juridiques',
                        'Prise de rendez-vous clients',
                        'Saisie de donn√©es juridiques'
                    ],
                    'annees_experience': 1,
                    'secteur': 'juridique'
                },
                'jobs': [
                    {
                        'id': 'manager_equipe_001',
                        'titre': 'Manager d\'√©quipe',
                        'competences': ['Management', 'Leadership', 'Gestion √©quipe', 'Objectifs'],
                        'missions': [
                            'Encadrement d\'une √©quipe de 8 personnes',
                            'D√©finition des objectifs',
                            'Suivi de performance',
                            'Animation de r√©unions'
                        ],
                        'secteur': 'management',
                        'description': 'Manager op√©rationnel avec √©quipe'
                    }
                ],
                'expected_v3_score_max': 25,
                'expected_v2_score_issue': 'high'
            },
            {
                'name': 'Gestionnaire_Paie_vs_Directeur',
                'description': 'üéØ Gestionnaire de paie ne doit PAS √™tre consid√©r√© comme Directeur (management)',
                'candidate': {
                    'titre_poste': 'Gestionnaire de Paie ADP',
                    'competences': ['ADP', 'Paie', 'Silae', 'Charges sociales'],
                    'missions': [
                        'Gestionnaire de paie exp√©riment√© ADP',
                        'Traitement paie multi-conventions',
                        'Formation utilisateurs Silae'
                    ],
                    'annees_experience': 5,
                    'secteur': 'rh'
                },
                'jobs': [
                    {
                        'id': 'directeur_001',
                        'titre': 'Directeur R√©gional',
                        'competences': ['Direction', 'Strat√©gie', 'Management', 'P&L'],
                        'missions': [
                            'Direction d\'une r√©gion de 50 personnes',
                            'D√©finition de la strat√©gie r√©gionale',
                            'Gestion du P&L',
                            'D√©veloppement commercial'
                        ],
                        'secteur': 'direction',
                        'description': 'Directeur avec responsabilit√© P&L'
                    }
                ],
                'expected_v3_score_max': 20,
                'expected_v2_score_issue': 'high'
            },
            {
                'name': 'Comptable_vs_Assistant_Facturation',
                'description': '‚úÖ CAS POSITIF : Comptable DOIT bien matcher avec Assistant facturation',
                'candidate': {
                    'titre_poste': 'Comptable',
                    'competences': ['Comptabilit√© g√©n√©rale', 'Sage', 'Facturation', 'TVA'],
                    'missions': [
                        'Comptabilit√© g√©n√©rale',
                        'Saisie factures fournisseurs',
                        'Rapprochements bancaires',
                        'D√©clarations TVA'
                    ],
                    'annees_experience': 2,
                    'secteur': 'comptabilit√©'
                },
                'jobs': [
                    {
                        'id': 'assistant_facturation_002',
                        'titre': 'Assistant Facturation',
                        'competences': ['Facturation', 'Sage', 'Clients', 'Excel'],
                        'missions': [
                            '√âmission factures',
                            'Suivi encaissements',
                            'Saisie comptable'
                        ],
                        'secteur': 'comptabilit√©',
                        'description': 'Assistant facturation avec comptabilit√©'
                    }
                ],
                'expected_v3_score_min': 70,  # Doit √™tre √©lev√©
                'positive_case': True
            },
            {
                'name': 'Gestionnaire_Paie_vs_Gestionnaire_Paie',
                'description': '‚úÖ CAS PARFAIT : M√™me m√©tier doit avoir score maximum',
                'candidate': {
                    'titre_poste': 'Gestionnaire de Paie',
                    'competences': ['Sage Paie', 'DADS', 'Urssaf'],
                    'missions': ['Gestion paie mensuelle'],
                    'annees_experience': 3,
                    'secteur': 'rh'
                },
                'jobs': [
                    {
                        'id': 'gestionnaire_paie_001',
                        'titre': 'Gestionnaire de Paie Senior',
                        'competences': ['Sage Paie', 'DADS', 'Urssaf', 'Formation'],
                        'missions': ['Gestion paie', 'Formation √©quipe'],
                        'secteur': 'rh',
                        'description': 'Gestionnaire paie avec formation'
                    }
                ],
                'expected_v3_score_min': 90,
                'perfect_case': True
            }
        ]
    
    def test_health_api(self) -> bool:
        """Teste la disponibilit√© de l'API V3.0"""
        try:
            response = requests.get(f"{self.api_base}/health", timeout=10)
            if response.status_code == 200:
                data = response.json()
                print(f"‚úÖ API V3.0 disponible - Version: {data.get('version', 'unknown')}")
                return True
            else:
                print(f"‚ùå API non disponible - Status: {response.status_code}")
                return False
        except Exception as e:
            print(f"‚ùå Erreur de connexion API: {e}")
            return False
    
    def test_job_analysis_v3(self) -> Dict[str, Any]:
        """Teste le nouveau endpoint d'analyse m√©tier V3.0"""
        print("\nüîç Test de l'analyse m√©tier V3.0...")
        
        test_texts = [
            {
                'text': 'Gestionnaire de paie avec 3 ans d\'exp√©rience Sage Paie URSSAF',
                'expected_job': 'gestionnaire_paie',
                'expected_sector': 'comptabilite_finance'
            },
            {
                'text': 'Assistant facturation relances clients recouvrement',
                'expected_job': 'assistant_facturation', 
                'expected_sector': 'comptabilite_finance'
            },
            {
                'text': 'Manager d\'√©quipe leadership encadrement 10 personnes',
                'expected_job': 'manager',
                'expected_sector': 'management_direction'
            }
        ]
        
        results = {}
        
        for i, test in enumerate(test_texts):
            try:
                response = requests.post(
                    f"{self.api_base.replace('/v1', '')}/v3.0/job-analysis",
                    json={'text': test['text'], 'context': 'cv'},
                    timeout=10
                )
                
                if response.status_code == 200:
                    data = response.json()
                    analysis = data.get('enhanced_analysis_v3', {})
                    
                    detected_job = analysis.get('specific_job', 'unknown')
                    detected_sector = analysis.get('primary_sector', 'unknown')
                    confidence = analysis.get('confidence', 0)
                    
                    success = (detected_job == test['expected_job'] and 
                              detected_sector == test['expected_sector'])
                    
                    results[f"test_{i+1}"] = {
                        'text': test['text'],
                        'expected_job': test['expected_job'],
                        'detected_job': detected_job,
                        'expected_sector': test['expected_sector'],
                        'detected_sector': detected_sector,
                        'confidence': confidence,
                        'success': success
                    }
                    
                    status = "‚úÖ" if success else "‚ùå"
                    print(f"  {status} Test {i+1}: {detected_job} ({confidence:.2f}) vs attendu {test['expected_job']}")
                    
                else:
                    print(f"  ‚ùå Test {i+1}: Erreur API - {response.status_code}")
                    
            except Exception as e:
                print(f"  ‚ùå Test {i+1}: Exception - {e}")
        
        return results
    
    def run_matching_test(self, test_case: Dict[str, Any], algorithm: str) -> Dict[str, Any]:
        """Ex√©cute un test de matching pour un algorithme donn√©"""
        try:
            payload = {
                'candidate': test_case['candidate'],
                'jobs': test_case['jobs'],
                'algorithm': algorithm,
                'options': {'include_details': True}
            }
            
            start_time = time.time()
            response = requests.post(f"{self.api_base}/match", json=payload, timeout=30)
            execution_time = (time.time() - start_time) * 1000
            
            if response.status_code == 200:
                data = response.json()
                matches = data.get('matches', [])
                
                if matches:
                    top_match = matches[0]
                    score = top_match.get('matching_score', 0)
                    
                    return {
                        'success': True,
                        'score': score,
                        'execution_time_ms': execution_time,
                        'algorithm_used': data.get('algorithm_used', algorithm),
                        'match_details': top_match.get('matching_details', {}),
                        'blocking_factors': top_match.get('blocking_factors', []),
                        'recommendations': top_match.get('recommendations', []),
                        'job_analysis_v3': top_match.get('job_analysis_v3', {}),
                        'raw_response': data
                    }
                else:
                    return {
                        'success': False,
                        'error': 'Aucun match retourn√©',
                        'execution_time_ms': execution_time
                    }
            else:
                return {
                    'success': False,
                    'error': f"API Error {response.status_code}: {response.text}",
                    'execution_time_ms': execution_time
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'execution_time_ms': 0
            }
    
    def compare_v2_vs_v3(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """Compare les r√©sultats V2.1 vs V3.0 pour un cas de test"""
        print(f"\nüî¨ Test: {test_case['name']}")
        print(f"üìù {test_case['description']}")
        
        # Test V2.1
        print("  üîπ Test Enhanced V2.1...")
        v2_result = self.run_matching_test(test_case, 'enhanced-v2')
        
        # Test V3.0
        print("  üîπ Test Enhanced V3.0...")
        v3_result = self.run_matching_test(test_case, 'enhanced-v3')
        
        # Analyse des r√©sultats
        comparison = {
            'test_name': test_case['name'],
            'description': test_case['description'],
            'v2_result': v2_result,
            'v3_result': v3_result,
            'improvement_analysis': {}
        }
        
        if v2_result['success'] and v3_result['success']:
            v2_score = v2_result['score']
            v3_score = v3_result['score']
            score_diff = v2_score - v3_score
            
            # Analyse selon le type de cas
            if test_case.get('positive_case'):
                # Cas positif : on veut un score √©lev√©
                expected_min = test_case.get('expected_v3_score_min', 70)
                v3_success = v3_score >= expected_min
                status = "‚úÖ" if v3_success else "‚ùå"
                print(f"    {status} V3.0 Score: {v3_score:.1f}% (attendu ‚â•{expected_min}%)")
                
            elif test_case.get('perfect_case'):
                # Cas parfait : on veut un score tr√®s √©lev√©
                expected_min = test_case.get('expected_v3_score_min', 90)
                v3_success = v3_score >= expected_min
                status = "‚úÖ" if v3_success else "‚ùå"
                print(f"    {status} V3.0 Score: {v3_score:.1f}% (attendu ‚â•{expected_min}%)")
                
            else:
                # Cas probl√©matique : on veut un score faible
                expected_max = test_case.get('expected_v3_score_max', 30)
                v3_success = v3_score <= expected_max
                improvement = score_diff > 0  # V3 doit √™tre plus bas que V2
                
                status = "‚úÖ" if v3_success else "‚ùå"
                improvement_status = "üìà" if improvement else "üìâ"
                
                print(f"    V2.1 Score: {v2_score:.1f}%")
                print(f"    {status} V3.0 Score: {v3_score:.1f}% (attendu ‚â§{expected_max}%)")
                print(f"    {improvement_status} Am√©lioration: {score_diff:+.1f} points")
                
                comparison['improvement_analysis'] = {
                    'score_reduction': score_diff,
                    'meets_expectation': v3_success,
                    'improvement_achieved': improvement,
                    'expected_max_score': expected_max
                }
        
        else:
            print("    ‚ùå Erreur dans un des tests")
            if not v2_result['success']:
                print(f"      V2.1 Error: {v2_result.get('error', 'Unknown')}")
            if not v3_result['success']:
                print(f"      V3.0 Error: {v3_result.get('error', 'Unknown')}")
        
        return comparison
    
    def run_performance_test(self) -> Dict[str, Any]:
        """Test des performances avec plusieurs requ√™tes"""
        print("\n‚ö° Test de performance...")
        
        # Utilise le premier cas de test pour mesurer les performances
        test_case = self.problematic_test_cases[0]
        algorithms_to_test = ['enhanced-v2', 'enhanced-v3']
        
        performance_results = {}
        
        for algorithm in algorithms_to_test:
            print(f"  üîπ Performance {algorithm}...")
            
            times = []
            scores = []
            
            # Fait 5 requ√™tes pour avoir une moyenne
            for i in range(5):
                result = self.run_matching_test(test_case, algorithm)
                if result['success']:
                    times.append(result['execution_time_ms'])
                    scores.append(result['score'])
            
            if times:
                avg_time = sum(times) / len(times)
                avg_score = sum(scores) / len(scores)
                
                performance_results[algorithm] = {
                    'avg_execution_time_ms': avg_time,
                    'avg_score': avg_score,
                    'times': times,
                    'scores': scores
                }
                
                print(f"    ‚è±Ô∏è  Temps moyen: {avg_time:.1f}ms")
                print(f"    üìä Score moyen: {avg_score:.1f}%")
        
        return performance_results
    
    def generate_validation_report(self) -> str:
        """G√©n√®re un rapport de validation d√©taill√©"""
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        
        # Calcul des statistiques globales
        total_tests = len(self.results['v2_vs_v3_comparison'])
        successful_improvements = 0
        problematic_cases_fixed = 0
        
        for comparison in self.results['v2_vs_v3_comparison']:
            if comparison['v2_result']['success'] and comparison['v3_result']['success']:
                # Cherche le cas de test correspondant
                test_case = next((tc for tc in self.problematic_test_cases 
                                if tc['name'] == comparison['test_name']), None)
                
                if test_case and not test_case.get('positive_case', False) and not test_case.get('perfect_case', False):
                    # Cas probl√©matique
                    v3_score = comparison['v3_result']['score']
                    expected_max = test_case.get('expected_v3_score_max', 30)
                    
                    if v3_score <= expected_max:
                        problematic_cases_fixed += 1
                    
                    v2_score = comparison['v2_result']['score']
                    if v2_score > v3_score:
                        successful_improvements += 1
        
        report = f"""
üéØ RAPPORT DE VALIDATION SUPERSMARTMATCH V3.0
================================================================
G√©n√©r√© le: {datetime.now().strftime("%d/%m/%Y √† %H:%M:%S")}

üìä R√âSUM√â EX√âCUTIF
----------------------------------------------------------------
‚úÖ Tests ex√©cut√©s: {total_tests}
üéØ Am√©liorations r√©ussies: {successful_improvements}/{total_tests}
üîß Cas probl√©matiques r√©solus: {problematic_cases_fixed}
üìà Taux de r√©ussite: {(successful_improvements/max(total_tests,1)*100):.1f}%

üéØ VALIDATION DES PROBL√àMES IDENTIFI√âS
----------------------------------------------------------------
"""
        
        for comparison in self.results['v2_vs_v3_comparison']:
            report += f"\nüìã {comparison['test_name'].replace('_', ' ')}\n"
            report += f"   üìù {comparison['description']}\n"
            
            if comparison['v2_result']['success'] and comparison['v3_result']['success']:
                v2_score = comparison['v2_result']['score']
                v3_score = comparison['v3_result']['score']
                score_diff = v2_score - v3_score
                
                # Trouve le cas de test pour les attentes
                test_case = next((tc for tc in self.problematic_test_cases 
                                if tc['name'] == comparison['test_name']), None)
                
                if test_case:
                    if test_case.get('positive_case') or test_case.get('perfect_case'):
                        expected_min = test_case.get('expected_v3_score_min', 70)
                        success = v3_score >= expected_min
                        status = "‚úÖ R√âUSSI" if success else "‚ùå √âCHEC"
                        report += f"   {status}: Score V3.0 = {v3_score:.1f}% (attendu ‚â•{expected_min}%)\n"
                    else:
                        expected_max = test_case.get('expected_v3_score_max', 30)
                        success = v3_score <= expected_max
                        improvement = score_diff > 0
                        
                        status = "‚úÖ R√âSOLU" if success else "‚ùå NON R√âSOLU"
                        report += f"   V2.1: {v2_score:.1f}% ‚Üí V3.0: {v3_score:.1f}% (attendu ‚â§{expected_max}%)\n"
                        report += f"   {status}: Am√©lioration de {score_diff:+.1f} points\n"
            else:
                report += "   ‚ùå ERREUR: Test non ex√©cutable\n"
        
        # Section performance
        if 'performance_metrics' in self.results and self.results['performance_metrics']:
            report += "\n‚ö° PERFORMANCES\n"
            report += "----------------------------------------------------------------\n"
            
            for algo, metrics in self.results['performance_metrics'].items():
                report += f"{algo}: {metrics['avg_execution_time_ms']:.1f}ms (score moyen: {metrics['avg_score']:.1f}%)\n"
        
        # Recommandations
        report += "\nüí° RECOMMANDATIONS\n"
        report += "----------------------------------------------------------------\n"
        
        if problematic_cases_fixed >= 3:
            report += "‚úÖ V3.0 r√©sout efficacement les probl√®mes de pr√©cision identifi√©s\n"
            report += "‚úÖ D√©ploiement en production recommand√©\n"
            report += "‚úÖ Mise √† jour des clients vers enhanced-v3\n"
        elif problematic_cases_fixed >= 2:
            report += "‚ö†Ô∏è Am√©liorations significatives mais perfectibles\n"
            report += "üîß Ajustements recommand√©s avant production\n"
        else:
            report += "‚ùå Probl√®mes de pr√©cision non r√©solus\n"
            report += "üîß R√©vision de l'algorithme V3.0 n√©cessaire\n"
        
        report += f"\nüìà Utiliser l'algorithme 'enhanced-v3' pour la pr√©cision m√©tier optimale\n"
        report += f"üîÑ Endpoint V3.0: /api/v3.0/job-analysis pour analyse m√©tier fine\n"
        
        return report
    
    def run_full_validation(self) -> None:
        """Ex√©cute la validation compl√®te V3.0"""
        print("üöÄ VALIDATION SUPERSMARTMATCH V3.0 - PR√âCISION M√âTIER FINE")
        print("=" * 70)
        
        # 1. Test de l'API
        if not self.test_health_api():
            print("‚ùå Validation annul√©e: API non disponible")
            return
        
        # 2. Test de l'analyse m√©tier V3.0
        job_analysis_results = self.test_job_analysis_v3()
        
        # 3. Tests de matching V2.1 vs V3.0
        print(f"\nüî¨ Comparaison V2.1 vs V3.0 sur {len(self.problematic_test_cases)} cas probl√©matiques...")
        
        for test_case in self.problematic_test_cases:
            comparison = self.compare_v2_vs_v3(test_case)
            self.results['v2_vs_v3_comparison'].append(comparison)
        
        # 4. Test de performance
        performance_metrics = self.run_performance_test()
        self.results['performance_metrics'] = performance_metrics
        
        # 5. G√©n√©ration du rapport
        print("\nüìä G√©n√©ration du rapport de validation...")
        report = self.generate_validation_report()
        
        # Sauvegarde du rapport
        report_filename = f"validation_v3_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(report)
        print(f"\nüìÑ Rapport sauvegard√©: {report_filename}")
        
        # Sauvegarde des r√©sultats d√©taill√©s
        results_filename = f"validation_v3_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(results_filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"üìÑ R√©sultats d√©taill√©s sauvegard√©s: {results_filename}")

def main():
    """Point d'entr√©e principal"""
    print("üéØ SuperSmartMatch V3.0 - Validation des Am√©liorations de Pr√©cision")
    print("Objectif: Valider la r√©solution des probl√®mes de matching identifi√©s")
    print()
    
    # Configuration
    api_base = "http://localhost:5061/api/v1"  # Port V3.0
    
    # V√©rification pr√©alable
    print("‚öôÔ∏è Configuration:")
    print(f"   API Base: {api_base}")
    print(f"   Tests: Cas probl√©matiques V2.1 vs am√©liorations V3.0")
    print()
    
    # Lancement de la validation
    validator = SuperSmartMatchV3Validator(api_base)
    validator.run_full_validation()

if __name__ == "__main__":
    main()
